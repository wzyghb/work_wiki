# cs231n cheat sheat

## Tranditional ML Method

## Tranditional NN

### Weight Initialization

+ 初始值为何不能全部设置为 0？此时所有的梯度计算都会给出为 0，类似的如果初始化为相同的值，会面临非对称性丧失的问题。 symmetry breaking
+ 一般将权重的值设置为小的随机数，我们猜测权重有大有小，有负有正，因而将其值设为绝对值很小的整数。 `np.randome.randn(D, H)` -> 使用了单位高斯分布的随机数，也可以使用 uniform distribution
+ 正则化参数：以上方法初始化的神经网络会产生一个由参数数量引起的 variance。可以启发式地初始化每个参数为： `w = np.random.randn(n) / sqrt(n)` 其中 n 是输入的数量。（证明可以参见 cs231n，初始化 weight 使得初始神经网络具有和目标输出网络类似的分布）
+ [Glorot paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 推荐设置 w 使得输出为 `Var(w) = 2 / (n_in + n_out)`
+ [He paper](https://arxiv.org/pdf/1502.01852.pdf) 推荐设置为：**`w = np.random.randn(n) * sqrt(2.0/n)`** 推荐用户使用 ReLU 神经网络时采用。

+ Sparse initialization：设置 weight 为 0，但是连接状态是随机生成的。
+ initializing the biases：一般设置为 0，目前没有明确的研究。

### Regularization

+ L2 Regularization: diffuse， small numbers weight, 一般情况下相比于 L1 更加优越
+ L1 Regularization: weight vector become sparse
+ [Elastic net regularization](http://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf)
+ Max norm constraints：限定每个 weight 绝对值的最大值。
+ [Dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) 仅以某个概率保持连接，否则会断开神经网络单元之间的连接。

dropout 的例子：
```python
""" 
Inverted Dropout: Recommended implementation example.
We drop and scale at train time and don't do anything at test time.
"""

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
  H1 *= U1 # drop! 对隐层单元进行dropout
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
  H2 *= U2 # drop! 对输出单元进行 dropout
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```

每次训练时随机地更新参数，但是在测试时我们需要完整的参数，因而此处训练时，要除以 p 进行参数归一化。否则需要 test 时也对 x 乘以 p，来是的测试时的结果和训练时有相同的度量衡。

+ [dropout training as adaptive regularization](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)

在前向计算中引入随机的 noise 来训练神经网络的主题。

Per-layer regularization: 通常不会再除去 output 以外的层使用不同的 regularization 方法。

In pratice：single、global L2 regularization strength。combine with dropout，p = 0.5 是一个通常的选择。

### Loss Function

regularazatiion 可以认为是复杂模型的惩罚，而 data loss 通常认为是预测值和真实值之间的误差。

```sh
L = 1 / N sum(i, L_i)
f = f(x_i; W)

# in SVM
L_i = sum(j != y_i, max(0, f_j - f_y_i + 1))
# some people report performance better squared hinge loss
L_i = sum(j != y_i, max(0, f_j - f_y_i + 1)**2)

# Softmax classifier:
L_i = -log( (exp(f_y_i) / sum(j, exp(f_j))))
```

Problem: Large number of classes: 当标签集合比较巨大时，（如英语字典或者 ImageNet）可以使用 [Hierarchical Softmax](https://arxiv.org/pdf/1310.4546.pdf), 其将label 分解为一个 tree，每个 label 都被表示为 tree 上的一个路径，训练模型的目的是消除每个节点左右子树的歧义。

Attribute classification：如果 yi 不是一个单独的标签，而是表示是否拥有某种属性，如图像中包括一组集合中的某几种物品，这时候，可以启发式地分为多个二分类问题进行处理，具有下面的形式：

```sh
L_i = sum(j, max(0, 1 - y_i_j * f_j))
```

j 表示某个分类，y_i_j 表示第 i 个物品是否属于 j 分类，是 1 或者 -1。

Regression

+ Regression 一般使用 L2 正则化，或者 L1 正则化，注意 L1 的梯度计算。
+ L2 相比于 softmax 更加难以计算，因为其对异常点敏感，（Softmax 实际上并不关心 score 本身的大小）当我们遇到一个 regression 任务时，我们首先要考虑这是否是必要的，然后再将你的输出转化为二进制数据。

### Structured Prediction

这种预测主要针对一些特别的 label 结构，如 graph、trees、其他复杂 object。

Structured SVM loss 的基本想法是计算一个正确的 structure yi 和一个高分数不正确结构之间的差距。通用的随机梯度下降算法不一定能够解决这种问题，一般需要特别的 solvers。

### Gradient check

通常使用的公式（二阶精度）：

```sh
df(x)/dx = (f(x+h) - f(x-h)) / (2h)
```

gradient check 时要使用相对的精度进行比较，公式如下：

```sh
|fa - fn| / max(|fa|, |fn|)
```

+ 上面这种形式不同于通常的相对误差，但避免了除零风险，并具有对称性。
+ 一般这种情况下最好使用 double 精度的数据。

精度验证结果的经验手段：

1. relative error > 1e-2 表示发生了计算错误
1. 1e-2 > re > 1e-4 结果不太好
1. 1e-4 > re 可以接受
1. 1e-7 > re 非常好
1. 对于一个大于十层的网络，1e-2 说不定也是 ok 的。

+ 注意浮点数的取值范围，[浮点数介绍](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)
+ 机器学习算法实现时很容易遇到这种情况，如每个 point 的 gradient 很小，这时候，使用该梯度叠加的数据点也会比较小，这将会导致更多的数值问题。最好每次打印原始的 `numerical / analytic` 的梯度，确保你比较的数字不会非常得小，不超过 1e-10。
+ Kinks （不可微的部分）x-h ~ x + h 正好包含了其中不可微的部分。唯一解决的办法是跟踪所有 max(x, y) 中的 winner，当发现正好有kinks 时说明这个梯度结果不准确。
+ 因为有 kinks 的存在， gradient check 时最好使用较少的 datapoints
+ h 值得选择也很重要，如果 h 值过小，会有数值问题，h 值一般选择 1e-6~1e-4
+ 某些情况下，任意选择测试点时并不能够证明梯度
+ 由于 loss function 的值实际上是由 data loss 和 regularization loss 两部分组成，有可能 regularization loss 影响过大，因而最好在验证时关掉
+ 最好关掉 dropout 和 augmentation
+ 一般只需要检查几个维度即可

Before learning：清楚的检查

+ 正确的 loss 值： CIFAR 中，我们假设每个类都有 1/10 的可能性，因而， `-ln(0.1) = 2.302`
+ 最开始可以先用小的数据集（比如 20 个数据来训练，确保可以到达 0 损失）
+ 使用 learning curve 来观察训练的过程。曲线的震动和 batch size 有关系，如果为 1 那么会较大，如果为 full dataset，会是最小的
+ 如果将 learning urver 以 log 曲线给出，将会类似于一条直线
+ 

Parameter Update

+ SGD
  + Vanilla update
  + Momentum update
  + Nesterov Momentum

Anneling the learning rate

+ step decay: + 0.1 ~ every 20
+ Exponential decay: `a = a0 * exp(-k*t)`
+ 1 / t decay:  `a = a0 / (1 + kt)`

Per-parameter adaptive learning rate methods

+ Adagrad
+ RMSprop
+ Adam

Hyperparameter optimization

+ Implementation: worker, master
+ Prefer one validation fold to cross-validation
+ Hyperparameter ranges: `learning_rate = 10 ** uniform(-6, 1)`
+ Prefer random search to grid search
+ 如果在边界上取得最优结果，要注意重新检验下
+ Stage 可以从粗糙到精确逐渐处理
+ Bayesian Hyperparameter Optimization: exploration - exploitation trade-off.

Model Ensembles

悬链几个不同的模型并将他们组合可以极好的提升预测的效果，一般会 typically monotonicaly improves。

+ Same model， different initializations
+ Top models discovered during cross-validation
+ Different checkpoints of a single model
+ Running average of parameters during training
